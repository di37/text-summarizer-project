TrainingArguments:
  num_train_epochs: 5 
  warmup_steps: 500
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  weight_decay=: .01 
  logging_steps: 100
  evaluation_strategy: steps
  eval_steps: 500 
  save_steps: 500
  gradient_accumulation_steps: 16
  gradient_checkpointing: True
  fp16: True
  optim: adafactor
  learning_rate: 3e-4